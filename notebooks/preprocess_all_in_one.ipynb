{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516565d1",
   "metadata": {},
   "source": [
    "This notebook takes the raw/downloaded information and pre-process it into a format suitable for AI/ML approaches. This pre-processing procedure assumes all gridded data is in the same spatio-temporal resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826db32",
   "metadata": {},
   "source": [
    "# Outcome variable (or predictand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08681ba",
   "metadata": {},
   "source": [
    "## Above Ground Biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load AGB data into an xarray dataset\"\"\"\n",
    "agb_data = xr.open_dataset(\"/data1/raw_data/veg_2010_2016/all_veg_data.nc\")\n",
    "agb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349a134",
   "metadata": {},
   "source": [
    "Now select below which data variable you would like to use as outcome, this will be used to mask all the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d865a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We suggest to select one of the maps by Avitabile et al.\n",
    "# This is the outcome variable and also works as mask for all the features.\n",
    "# All the features geerated from here onwards, will depend on this choice!\n",
    "vartype = \"mean\"  # it can also be '05th' or '95th'\n",
    "varname = \"abg_avitabile_vod\" + vartype\n",
    "\n",
    "agb_data = eval(\"agb_data.\" + varname)\n",
    "# Units\n",
    "agb_data.units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec3ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b197b",
   "metadata": {},
   "source": [
    "Please note the units of AGB are Mg/h!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b657ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agb_data[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4db7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the predictors (VOD) is available from April 2010 to December 2016.\n",
    "# Therefore here we remove Jan-Feb-Mar 2016.\n",
    "agb_data = agb_data.loc[\"2010-04-01\":\"2016-12-31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc726c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out where AGB is zero\n",
    "AGB_THRESH = 0\n",
    "\n",
    "agb_data = agb_data.where(agb_data != AGB_THRESH)\n",
    "agb_data[0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211b25d",
   "metadata": {},
   "source": [
    "## Burned Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load BA data into an xarray dataset\"\"\"\n",
    "ba_data = xr.open_mfdataset(\n",
    "    \"/data1/raw_data/burned_area_2010_2018/201[0-6]*-ESACCI-L4_FIRE-BA-MODIS-fv5.1.nc\"\n",
    ")\n",
    "# Units\n",
    "ba_data.burned_area.units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f8660",
   "metadata": {},
   "source": [
    "Please note the units of BA are m2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename lat/lon dimensions\n",
    "ba_data = ba_data.rename({\"lon\": \"longitude\", \"lat\": \"latitude\"})\n",
    "# Extract only burned areas\n",
    "ba_data = ba_data.burned_area\n",
    "\n",
    "ba_data[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert m2 to hectares\n",
    "ba_data_h = ba_data * 0.0001\n",
    "ba_data_h[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the predictors (VOD) is available from April 2010 to December 2016.\n",
    "# Therefore here we remove Jan-Feb-Mar 2016.\n",
    "ba_data_h = ba_data_h.loc[\"2010-04-01\":\"2016-12-31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out low values (small fires)\n",
    "BA_THRESH = 50  # hectares\n",
    "ba_data_h = ba_data_h.where(ba_data_h > BA_THRESH)\n",
    "\n",
    "ba_data_h[0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7b5bc",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "The load (Mg) is the Above Ground Biomass (Mg/h) * Burned Area (h). This operation is straightforward because the grids have the same spatial and temporal resolution! Please note the result shows values where both BA and AGB are not equal to NA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = agb_data * ba_data_h\n",
    "load_data[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data, if needed.\n",
    "folder_path = \"/home/moc0/ai-vegetation-fuel/data/inputs/nc_files_\" + vartype + \"/\"\n",
    "Path(folder_path).mkdir(parents=True, exist_ok=True)\n",
    "load_data.to_netcdf(folder_path + \"load_2010-2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0c9ac",
   "metadata": {},
   "source": [
    "# Static predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe89ecd",
   "metadata": {},
   "source": [
    "## Climatic regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d141b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Load climatic regions data into an xarray dataset\"\"\"\n",
    "cr_data = xr.open_dataset(\"/data1/raw_data/Beck_KG_V1_present_0p0083.gridName0320.nc\")\n",
    "# Rotate longitude coordinates\n",
    "cr_data = cr_data.assign_coords(\n",
    "    longitude=(((cr_data.longitude + 180) % 360) - 180)\n",
    ").sortby(\"longitude\")\n",
    "# Interpolate to match AGB resolution\n",
    "climate_region = cr_data.climatic_region.interp(\n",
    "    coords={\n",
    "        \"latitude\": agb_data.latitude.values,\n",
    "        \"longitude\": agb_data.longitude.values,\n",
    "    },\n",
    "    method=\"nearest\",\n",
    ")  # Wikilimo used default method ('linear')\n",
    "climate_region.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da835da3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replicate the same layer 81 times to match load's temporal resolution\n",
    "cr_data = [climate_region for i in range(0, 81)]\n",
    "cr_data = xr.concat(cr_data, \"time\")\n",
    "cr_data[\"time\"] = load_data[\"time\"]\n",
    "# Mask using the load\n",
    "cr_data = cr_data.where(load_data >= 0)\n",
    "cr_data[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "cr_data.to_netcdf(folder_path + \"climatic_region_2010-2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21253f10",
   "metadata": {},
   "source": [
    "## Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8795f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load slope data into an xarray dataset\"\"\"\n",
    "slope_data = xr.open_mfdataset(\"/data1/raw_data/slope_O320.nc\")\n",
    "# Rotate longitude coordinates\n",
    "slope_data = slope_data.slor.assign_coords(\n",
    "    longitude=(((slope_data.longitude + 180) % 360) - 180)\n",
    ").sortby(\"longitude\")\n",
    "# Interpolate to match AGB resolution\n",
    "slope_data = slope_data.interp(\n",
    "    coords={\n",
    "        \"latitude\": agb_data.latitude.values,\n",
    "        \"longitude\": agb_data.longitude.values,\n",
    "    },\n",
    "    method=\"linear\",\n",
    ")  # Wikilimo used default method ('linear')\n",
    "slope_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the same layer 81 times to match load's temporal resolution\n",
    "sl_data = [slope_data for i in range(0, 81)]\n",
    "sl_data = xr.concat(sl_data, \"time\")\n",
    "sl_data[\"time\"] = load_data[\"time\"]\n",
    "# Mask using the load\n",
    "sl_data = sl_data.where(load_data >= 0)\n",
    "sl_data[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "sl_data.to_netcdf(folder_path + \"slope_2010-2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56e546",
   "metadata": {},
   "source": [
    "## Biomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load slope data into an xarray dataset\"\"\"\n",
    "biomes = xr.open_dataset(\"/data1/downloaded/landcover_25.nc\")\n",
    "# Convert to data array and select the only time step available\n",
    "biomes = biomes.STRF[0]\n",
    "biomes.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the same layer 81 times to match load's temporal resolution\n",
    "biomes_data = [biomes for i in range(0, 81)]\n",
    "biomes_data = xr.concat(biomes_data, \"time\")\n",
    "biomes_data[\"time\"] = load_data[\"time\"]\n",
    "# Mask using the load\n",
    "biomes_data = biomes_data.where(load_data >= 0)\n",
    "biomes_data[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "biomes_data.to_netcdf(folder_path + \"biomes_2010-2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa13e2f",
   "metadata": {},
   "source": [
    "# Dynamic predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0f55fc",
   "metadata": {},
   "source": [
    "## Leaf Area Index\n",
    "\n",
    "Remote sensing LAI methods generate a map of dimensionless LAI values assigned to each pixel. Values can range from 0 (bare ground) to 6 or more, but since rangeland vegetation is generally sparse, values commonly range from 0-1. A LAI value of 1 means that there is the equivalent of 1 layer of leaves that entirely cover a unit of ground surface area, and less than one means that there is some bare ground between vegetated patches. LAI values over 1 indicate a layered canopy with multiple layers of leaves per unit ground surface area. LAI and fPAR data are commonly packaged together (e.g., MODIS products)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load LAI data into an xarray dataset\"\"\"\n",
    "lai_data = xr.open_mfdataset(\n",
    "    \"/data1/raw_data/LAI_interpolated_2010_2017/LAI_201[0-6]*.nc\"\n",
    ")\n",
    "# Rename lat/lon dimensions\n",
    "lai_data = lai_data.LAI.rename({\"lon\": \"longitude\", \"lat\": \"latitude\"})\n",
    "# Calculate monthly means\n",
    "lai_data = lai_data.resample(time=\"1MS\").mean(dim=\"time\")\n",
    "# Interpolate to match AGB resolution\n",
    "lai_data = lai_data.interp(\n",
    "    coords={\n",
    "        \"latitude\": agb_data.latitude.values,\n",
    "        \"longitude\": agb_data.longitude.values,\n",
    "    },\n",
    "    method=\"linear\",\n",
    ")  # Wikilimo used default method ('linear')\n",
    "# One of the predictors (VOD) is available from April 2010 to December 2016.\n",
    "# Therefore here we remove Jan-Feb-Mar 2016.\n",
    "lai_data = lai_data.loc[\"2010-04-01\":\"2016-12-31\"]\n",
    "lai_data[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask using the load\n",
    "lai_data = lai_data.where(load_data >= 0)\n",
    "lai_data[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "lai_data.to_netcdf(folder_path + \"lai_2010-2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c4177f",
   "metadata": {},
   "source": [
    "## Vegetation Optical Depth (VDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load VOD data into an xarray dataset\"\"\"\n",
    "vodfiles = [\n",
    "    os.path.join(d, x)\n",
    "    for year in range(2010, 2017)\n",
    "    for d, dirs, files in os.walk(\"/data1/downloaded/ESA_VOD/\" + str(year))\n",
    "    for x in files\n",
    "    if x.endswith(\".nc\")\n",
    "]\n",
    "vod_data = xr.open_mfdataset(vodfiles)\n",
    "# Select variable of interest: SM_IDW\n",
    "vod_data = vod_data.SM_IDW\n",
    "# Calculate monthly means\n",
    "vod_data = vod_data.resample(time=\"1MS\").mean(dim=\"time\")\n",
    "# Interpolate to match load's resolution\n",
    "vod_data = vod_data.interp(\n",
    "    coords={\n",
    "        \"latitude\": load_data.latitude.values,\n",
    "        \"longitude\": load_data.longitude.values,\n",
    "    },\n",
    "    method=\"linear\",\n",
    ")\n",
    "\n",
    "vod_data[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0186b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask using the load\n",
    "vod_data = vod_data.where(load_data >= 0)\n",
    "vod_data[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "vod_data.to_netcdf(folder_path + \"vod_2010_2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3378ef",
   "metadata": {},
   "source": [
    "## Standardised Precipitation Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load SPI data into an xarray dataset\"\"\"\n",
    "spi_data = xr.open_mfdataset(\"/data1/raw_data/SPI_GPCC/output_201[0-6]*.nc\")\n",
    "# Rotate longitude coordinates\n",
    "spi_data = spi_data.assign_coords(\n",
    "    longitude=(((spi_data.longitude + 180) % 360) - 180)\n",
    ").sortby(\"longitude\")\n",
    "# Interpolate to match load resolution\n",
    "spi_data = spi_data.interp(\n",
    "    coords={\n",
    "        \"latitude\": load_data.latitude.values,\n",
    "        \"longitude\": load_data.longitude.values,\n",
    "    },\n",
    "    method=\"linear\",\n",
    ")  # Wikilimo used default method ('linear')\n",
    "# One of the predictors (VOD) is available from April 2010 to December 2016.\n",
    "# Therefore here we remove Jan-Feb-Mar 2016.\n",
    "spi_data = spi_data.loc[dict(time=slice(\"2010-04-01\", \"2016-12-31\"))]\n",
    "spi_data.spi03[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix time stamps\n",
    "spi_data[\"time\"] = load_data[\"time\"]\n",
    "# Mask using the load\n",
    "spi_data = spi_data.where(load_data >= 0)\n",
    "spi_data.spi03[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "spi_data.to_netcdf(folder_path + \"spi_2010_2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac66773",
   "metadata": {},
   "source": [
    "## Weather Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Weather data into an xarray dataset\"\"\"\n",
    "wa_data = xr.open_mfdataset(\"/data1/raw_data/SEAS5_anomalies/S5_anomaly_201[0-6]*.nc\")\n",
    "# Rotate longitude coordinates\n",
    "wa_data = wa_data.assign_coords(\n",
    "    longitude=(((wa_data.longitude + 180) % 360) - 180)\n",
    ").sortby(\"longitude\")\n",
    "# Interpolate to match load resolution\n",
    "wa_data = wa_data.interp(\n",
    "    coords={\n",
    "        \"latitude\": load_data.latitude.values,\n",
    "        \"longitude\": load_data.longitude.values,\n",
    "    },\n",
    "    method=\"linear\",\n",
    ")  # Wikilimo used default method ('linear')\n",
    "# One of the predictors (VOD) is available from April 2010 to December 2016.\n",
    "# Therefore here we remove Jan-Feb-Mar 2016.\n",
    "wa_data = wa_data.loc[dict(time=slice(\"2010-04-01\", \"2016-12-31\"))]\n",
    "wa_data.d2m[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask using the load\n",
    "wa_data = wa_data.where(load_data >= 0)\n",
    "wa_data.d2m[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "wa_data.to_netcdf(folder_path + \"weather_anomalies_2010_2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490bdd01",
   "metadata": {},
   "source": [
    "## Fire anomalies (to be recalculated on Monday 23rd April 2021!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load fire data into an xarray dataset\"\"\"\n",
    "fa_data = xr.open_mfdataset(\n",
    "    \"/data1/raw_data/SEAS_FIRE_ANOMALIES_2010_2018/ECMWF_FWI_201[0-6]*_anom_m1.nc\"\n",
    ")\n",
    "# Rotate longitude coordinates\n",
    "fa_data = fa_data.assign_coords(\n",
    "    longitude=(((fa_data.longitude + 180) % 360) - 180)\n",
    ").sortby(\"longitude\")\n",
    "# Interpolate to match load resolution\n",
    "fa_data = fa_data.interp(\n",
    "    coords={\n",
    "        \"latitude\": load_data.latitude.values,\n",
    "        \"longitude\": load_data.longitude.values,\n",
    "    },\n",
    "    method=\"linear\",\n",
    ")  # Wikilimo used default method ('linear')\n",
    "# One of the predictors (VOD) is available from April 2010 to December 2016.\n",
    "# Therefore here we remove Jan-Feb-Mar 2016.\n",
    "fa_data = fa_data.loc[dict(time=slice(\"2010-04-01\", \"2016-12-31\"))]\n",
    "fa_data.fwinx[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask using load\n",
    "fa_data = fa_data.where(load_data >= 0)\n",
    "fa_data.fwinx[0].plot()\n",
    "\n",
    "# Store data, if needed.\n",
    "fa_data.to_netcdf(folder_path + \"fire_anomalies_2010-2016.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd56481",
   "metadata": {},
   "source": [
    "# Convert gridded information to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [\n",
    "    load_data.to_dataframe(name=\"fuel_load\").dropna(),  # outcome (unnamed) feature\n",
    "    cr_data.to_dataframe(name=\"climatic_region\").dropna(),  # static (unnamed) feature\n",
    "    sl_data.to_dataframe(name=\"slope\").dropna(),  # static (unnamed) feature\n",
    "    biomes_data.to_dataframe(name=\"biome\").dropna(),  # static (unnamed) feature\n",
    "    lai_data.to_dataframe(name=\"lai\").dropna(),  # dynamic (unnamed) feature\n",
    "    vod_data.to_dataframe(name=\"vod\").dropna(),  # dynamic (unnamed) feature\n",
    "    spi_data.to_dataframe().dropna(),  # dynamic (named) features\n",
    "    wa_data.to_dataframe().dropna(),  # dynamic (named) features\n",
    "    fa_data.to_dataframe().dropna(),  # dynamic (named) features\n",
    "]  \n",
    "\n",
    "# Perform an inner join to get rows without NAs\n",
    "df = pd.concat(frames, join=\"inner\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f577d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/inputs/fuel_load_and_features_dataframe.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
